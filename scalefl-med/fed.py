import copy
import datetime as dt
import os
import pickle as pkl

import numpy as np
import torch
import torch.multiprocessing as mp

from data_tools.dataloader import get_client_dataloader
from predict import local_validate, minigpt_validate
from train import execute_epoch
from utils.grad_traceback import get_downscale_index
from utils.utils import save_checkpoint

mp.set_start_method('spawn', force=True)


class Federator:
    def __init__(self, global_model, args, client_groups=[]):
        self.args = args
        self.global_model = global_model

        self.vertical_scale_ratios = args.vertical_scale_ratios
        self.horizontal_scale_ratios = args.horizontal_scale_ratios
        self.client_split_ratios = args.client_split_ratios

        assert len(self.vertical_scale_ratios) == len(self.horizontal_scale_ratios) == len(self.client_split_ratios)

        self.num_rounds = args.num_rounds
        self.num_clients = args.num_clients
        self.sample_rate = args.sample_rate
        self.alpha = args.alpha
        self.num_levels = len(self.vertical_scale_ratios)
        
        # æ¶æ„é€‚é…ï¼šMiniGPTå¤§æ¨¡å‹ä½¿ç”¨LoRAåˆ†å±‚é…ç½®ï¼Œä¼ ç»Ÿæ¨¡å‹ä½¿ç”¨æ¢¯åº¦è¿½è¸ª
        self.is_minigpt = 'MiniGPT' in str(type(global_model))
        
        if self.is_minigpt:
            print("=" * 60)
            print("ğŸ”§ æ£€æµ‹åˆ°MiniGPTæ¨¡å‹")
            print("=" * 60)
            print("âœ“ è·³è¿‡æ¢¯åº¦è¿½è¸ªï¼ˆé¿å…æ˜¾å­˜æº¢å‡ºï¼‰")
            print("âœ“ å¯ç”¨LoRAèµ„æºè‡ªé€‚åº”é…ç½®")
            print("-" * 60)
            
            # ä¸ºä¸åŒèµ„æºç­‰çº§å®šä¹‰LoRAé…ç½®ï¼ˆLevel-Adaptiveç­–ç•¥ï¼‰
            # é…ç½®æ ¼å¼: {'layers': è®­ç»ƒçš„å±‚åˆ—è¡¨, 'rank': LoRAç§©, 'alpha': LoRA alpha, 
            #           'lr': å­¦ä¹ ç‡, 'grad_clip': æ¢¯åº¦è£å‰ª, 'weight_decay': æƒé‡è¡°å‡}
            self.lora_configs = {
                0: {
                    'layers': list(range(8)),  
                    'rank': 8,     
                    'alpha': 16,   
                    'desc': 'ä½èµ„æº(å‰8å±‚)',
                    'lr': 1e-6,           # æä½å­¦ä¹ ç‡ï¼ˆ8å±‚æœ€ä¸ç¨³å®šï¼‰
                    'grad_clip': 1.5,     # æé«˜è£å‰ªé˜ˆå€¼
                    'weight_decay': 0.02  # å¼ºæ­£åˆ™åŒ–
                },
                1: {
                    'layers': list(range(16)),
                    'rank': 16,    
                    'alpha': 32,   
                    'desc': 'ä¸­ä½èµ„æº(å‰16å±‚)',
                    'lr': 2e-6,
                    'grad_clip': 2.5,
                    'weight_decay': 0.015
                },
                2: {
                    'layers': list(range(24)),
                    'rank': 32,    
                    'alpha': 64,   
                    'desc': 'ä¸­é«˜èµ„æº(å‰24å±‚)',
                    'lr': 3e-6,
                    'grad_clip': 4.0,
                    'weight_decay': 0.01
                },
                3: {
                    'layers': list(range(32)),
                    'rank': 64,    
                    'alpha': 128,  
                    'desc': 'å…¨èµ„æº(32å±‚)',
                    'lr': 5e-6,
                    'grad_clip': 5.0,  # æé«˜åˆ°5.0ï¼Œå…è®¸æ›´å¤§æ¢¯åº¦
                    'weight_decay': 0.01
                },
            }
            
            for level, config in self.lora_configs.items():
                print(f"  Level {level}: {config['desc']} - rank={config['rank']}, alpha={config['alpha']}")
            
            print("=" * 60)
            self.idx_dicts = None  # ä¸ä½¿ç”¨ç´¢å¼•å­—å…¸
        else:
            # ä¼ ç»Ÿæ¨¡å‹ä»ä½¿ç”¨æ¢¯åº¦è¿½è¸ª
            self.idx_dicts = [get_downscale_index(self.global_model, args, s) for s in self.vertical_scale_ratios]   
            self.lora_configs = None
            self.is_minigpt = False
        
        self.client_groups = client_groups

        self.lora_target_modules = getattr(global_model, "lora_target_modules", [])

        self.use_gpu = args.use_gpu

    def fed_train(self, train_set, val_set, user_groups, criterion, args, batch_size, train_params):

        scores = ['epoch\ttrain_loss\tval_loss\tval_acc1\tval_acc5\tlocal_val_acc1\tlocal_val_acc5' +
                  '\tlocal_val_acc1' * self.num_levels]
        best_acc1, best_round = 0.0, 0

        # å¦‚æœæ²¡æœ‰æä¾›å®¢æˆ·ç«¯åˆ†ç»„ï¼Œåˆ™æ ¹æ®å®¢æˆ·ç«¯åˆ†ç»„æ¯”ä¾‹éšæœºåˆ†é…å®¢æˆ·ç«¯ã€‚
        # pre-assignment of levels to clients (needs to be saved for inference)
        if not self.client_groups:
            client_idxs = np.arange(self.num_clients)
            np.random.seed(args.seed)
            shuffled_client_idxs = np.random.permutation(client_idxs)
            client_groups = []
            s = 0
            for i, ratio in enumerate(self.client_split_ratios):
                # æœ€åä¸€ç»„åŒ…å«æ‰€æœ‰å‰©ä½™å®¢æˆ·ç«¯
                if i == len(self.client_split_ratios) - 1:
                    e = len(shuffled_client_idxs)
                else:
                    e = s + int(len(shuffled_client_idxs) * ratio)
                client_groups.append(shuffled_client_idxs[s: e])
                s = e
            self.client_groups = client_groups
            
            # æ‰“å°å®¢æˆ·ç«¯åˆ†ç»„ä¿¡æ¯
            print("=" * 60)
            print("ğŸ“Š å®¢æˆ·ç«¯èµ„æºç­‰çº§åˆ†é…")
            print("=" * 60)
            for level, group in enumerate(client_groups):
                if self.is_minigpt:
                    config = self.lora_configs[level]
                    print(f"Level {level}: {len(group)}ä¸ªå®¢æˆ·ç«¯ â†’ {len(config['layers'])}å±‚LoRA, rank={config['rank']}")
                else:
                    print(f"Level {level}: {len(group)}ä¸ªå®¢æˆ·ç«¯ â†’ scale={self.vertical_scale_ratios[level]}")
            print("=" * 60)

            with open(os.path.join(args.save_path, 'client_groups.pkl'), 'wb') as f:
                pkl.dump(self.client_groups, f)

        # è¿›å…¥è®­ç»ƒè½®æ¬¡å¾ªç¯ï¼Œæ¯è½®è°ƒç”¨ execute_round æ–¹æ³•è¿›è¡Œè®­ç»ƒå’ŒéªŒè¯ã€‚
        # è®°å½•æ¯è½®çš„è®­ç»ƒæŸå¤±ã€éªŒè¯æŸå¤±å’Œå‡†ç¡®ç‡ï¼Œå¹¶ä¿å­˜æœ€ä½³æ¨¡å‹ã€‚
        # ã€ä¼˜åŒ–ã€‘åˆå§‹åŒ–best_val_lossç”¨äºMiniGPTæ¨¡å‹
        best_val_loss = float('inf')
        
        for round_idx in range(args.start_round, self.num_rounds):

            print(f'\n | Global Training Round : {round_idx + 1} |\n')

            train_loss, val_results, local_val_results = \
                self.execute_round(train_set, val_set, user_groups, criterion, args, batch_size,
                                   train_params, round_idx)

            # æ¯è½®ç»“æŸåå¼ºåˆ¶æ¸…ç†GPUç¼“å­˜ï¼Œé˜²æ­¢å†…å­˜ç´¯ç§¯
            torch.cuda.empty_cache()
            
            val_loss, val_acc1, val_acc5, _, _ = val_results

            scores.append(('{}' + '\t{:.4f}' * int(6 + self.num_levels))
                          .format(round_idx, train_loss, val_loss, val_acc1, val_acc5,
                                  local_val_results[-1][1], local_val_results[-1][2],
                                  *[l[1] for l in local_val_results[:-1]]))

            # ã€ä¼˜åŒ–ã€‘MiniGPTæ¨¡å‹åŸºäºval_lossåˆ¤æ–­æœ€ä½³æ¨¡å‹ï¼ˆlossè¶Šä½è¶Šå¥½ï¼‰ï¼Œä¼ ç»Ÿæ¨¡å‹åŸºäºval_acc1
            if self.is_minigpt:
                is_best = val_loss < best_val_loss
                if is_best:
                    best_val_loss = val_loss
                    best_round = round_idx
                    print(f'â­ æœ€ä½³éªŒè¯Loss: {best_val_loss:.4f} (Round {round_idx + 1})')
            else:
                is_best = val_acc1 > best_acc1
                if is_best:
                    best_acc1 = val_acc1
                    best_round = round_idx
                    print('Best var_acc1 {}'.format(best_acc1))

            model_filename = 'checkpoint_%03d.pth.tar' % round_idx
            # ã€ä¿®å¤ã€‘åªä¿å­˜LoRAå‚æ•°ï¼Œé¿å…ä¿å­˜27GBåŸºç¡€æ¨¡å‹
            if args.arch == 'MiniGPTv2':
                state_dict_to_save = {
                    k: v for k, v in self.global_model.state_dict().items()
                    if 'lora' in k.lower()
                }
                print(f"ğŸ’¾ ä¿å­˜LoRA checkpoint: {len(state_dict_to_save)}ä¸ªå‚æ•°")
            else:
                state_dict_to_save = self.global_model.state_dict()
            
            # ã€ä¼˜åŒ–ã€‘ä¿å­˜checkpointçŠ¶æ€ï¼ŒåŒ…å«val_lossä¿¡æ¯
            save_checkpoint({
                'round': round_idx,
                'arch': args.arch,
                'state_dict': state_dict_to_save,
                'best_acc1': best_acc1,
                'val_loss': val_loss,
                'best_val_loss': best_val_loss if self.is_minigpt else None,
            }, args, is_best, model_filename, scores)

        return best_acc1, best_round

    # æ ¹æ®å®¢æˆ·ç«¯ç´¢å¼•è¿”å›å…¶å¯¹åº”çš„å¤æ‚åº¦çº§åˆ«ã€‚å¦‚æœå®¢æˆ·ç«¯ä¸åœ¨ä»»ä½•åˆ†ç»„ä¸­ï¼Œåˆ™è¿”å› -1ã€‚
    def get_level(self, client_idx):
        # Return the complexity level of given client, starts with 0
        try:
            level = np.where([client_idx in c for c in self.client_groups])[0][0]
        except:
            # client will be skipped
            level = -1

        return level

    def execute_round(self, train_set, val_set, user_groups, criterion, args, batch_size, train_params, round_idx):
        self.global_model.train()
        m = max(int(self.sample_rate * self.num_clients), 1)
        client_idxs = np.random.choice(range(self.num_clients), m, replace=False)

        # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯è·å–æœ¬åœ°æ•°æ®åŠ è½½å™¨ã€å¤æ‚åº¦çº§åˆ«ã€ç¼©æ”¾æ¯”ä¾‹å’Œæœ¬åœ°æ¨¡å‹ã€‚
        client_train_loaders = [get_client_dataloader(train_set, user_groups[0][client_idx], args, batch_size) for
                                client_idx in client_idxs]
        levels = [self.get_level(client_idx) for client_idx in client_idxs]
        scales = [self.vertical_scale_ratios[level] for level in levels]
        local_models = [self.get_local_split(levels[i], scales[i]) for i in range(len(client_idxs))]
        h_scale_ratios = [self.horizontal_scale_ratios[level] for level in levels]

        pool_args = [train_set, user_groups, criterion, args, batch_size, train_params, round_idx]
        local_weights = []
        local_losses = []
        local_grad_flags = []

        pool_args.append(None)

        for i, client_idx in enumerate(client_idxs):
            # ä¼ é€’federatorçš„lora_configsç»™å®¢æˆ·ç«¯è®­ç»ƒå‡½æ•°
            client_args = pool_args + [local_models[i], client_train_loaders[i], levels[i], scales[i], 
                                       h_scale_ratios[i], client_idx, self.lora_configs]
            result = execute_client_round(client_args)

            if args.use_gpu:
                for k, v in result[0].items():
                    result[0][k] = v.cuda(0)

            local_weights.append(result[0])
            local_grad_flags.append(result[1])
            local_losses.append(result[2])
            print(f'Client {i+1}/{len(client_idxs)} completely finished')

        train_loss = sum(local_losses) / len(client_idxs)

        # Update the global model
        global_weights = self.average_weights(local_weights, local_grad_flags, levels, self.global_model)
        self.global_model.load_state_dict(global_weights)

        # Validation for all clients
        if self.client_split_ratios[-1] == 0:
            level = np.where(self.client_split_ratios)[0].tolist()[-1]
            scale = self.vertical_scale_ratios[level]
            global_model = self.get_local_split(level, scale)
            if self.use_gpu:
                global_model = global_model.cuda()
        else:
            # å¯¹äºMiniGPTå¤§æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨å¼•ç”¨ï¼Œé¿å…deepcopyå¯¼è‡´OOM
            if self.is_minigpt:
                global_model = self.global_model
            else:
                global_model = copy.deepcopy(self.global_model)

        # éªŒè¯é˜¶æ®µï¼šMiniGPTæ¨¡å‹ä½¿ç”¨ç®€åŒ–éªŒè¯ï¼ˆä»…è®¡ç®—Lossï¼‰
        if self.is_minigpt:
            print("=" * 60)
            print("ğŸ“Š MiniGPTæ¨¡å‹éªŒè¯ï¼ˆç®€åŒ–ç‰ˆï¼šä»…è®¡ç®—Lossï¼‰")
            print("=" * 60)
            
            # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨éƒ¨åˆ†éªŒè¯æ•°æ®ï¼‰
            # è¿™é‡Œç®€åŒ–ä¸ºä½¿ç”¨ä¸€ä¸ªå®¢æˆ·ç«¯çš„éªŒè¯æ•°æ®
            if len(user_groups[1]) > 0:
                val_client_idx = 0  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯çš„éªŒè¯æ•°æ®
                val_loader = get_client_dataloader(val_set, user_groups[1][val_client_idx], args, batch_size=1)
                
                try:
                    avg_val_loss = minigpt_validate(global_model, val_loader, criterion, args)
                except Exception as e:
                    print(f"âš ï¸  éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
                    avg_val_loss = 0.0
            else:
                print("âš ï¸  æ²¡æœ‰éªŒè¯æ•°æ®ï¼Œè·³è¿‡éªŒè¯")
                avg_val_loss = 0.0
            
            # è¿”å›å…¼å®¹çš„æ ¼å¼ï¼ˆä½¿ç”¨Lossä½œä¸ºä¸»è¦æŒ‡æ ‡ï¼‰
            val_results = (avg_val_loss, 0.0, 0.0, np.array([0.0]), np.array([0.0]))
            local_val_results = [(avg_val_loss, 0.0, 0.0) for _ in range(self.num_levels + 1)]
            print("=" * 60)
        else:
            # ä¼ ç»ŸCNNæ¨¡å‹ï¼šæ­£å¸¸æ‰§è¡ŒéªŒè¯
            val_results, local_val_results = local_validate(self, val_set, user_groups[1], criterion, args, 512,
                                                            global_model)

        # ã€å¼ºåŒ–å†…å­˜æ¸…ç†ã€‘é˜²æ­¢DataLoader workerè¿›ç¨‹æ³„æ¼
        # 1. æ˜¾å¼å…³é—­æ‰€æœ‰DataLoader
        for loader in client_train_loaders:
            # æ¸…ç†DataLoaderçš„è¿­ä»£å™¨å’Œworker
            if hasattr(loader, '_iterator') and loader._iterator is not None:
                try:
                    loader._iterator._shutdown_workers()
                except:
                    pass
                del loader._iterator
        
        # 2. åˆ é™¤æ‰€æœ‰ä¸´æ—¶å˜é‡
        del client_train_loaders, local_models, local_weights, local_losses, local_grad_flags
        if self.is_minigpt and 'val_loader' in locals():
            del val_loader
        
        # 3. å¼ºåˆ¶Pythonåƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        # 4. æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        
        return train_loss, val_results, local_val_results

    def average_weights(self, w, grad_flags, levels, model):
        """
        èšåˆå¤šä¸ªå®¢æˆ·ç«¯çš„æ¨¡å‹å‚æ•°
        - å¯¹äºMiniGPTï¼šä»…èšåˆLoRAå‚æ•°ï¼ˆç®€å•å¹³å‡ï¼‰
        - å¯¹äºä¼ ç»Ÿæ¨¡å‹ï¼šä½¿ç”¨åŸå§‹çš„æ¢¯åº¦æ„ŸçŸ¥èšåˆé€»è¾‘
        """
        # å¯¹äºMiniGPTå¤§æ¨¡å‹ï¼Œé¿å…deepcopyæ•´ä¸ªstate_dictï¼ˆä¼šOOMï¼‰
        # æ”¹ä¸ºç›´æ¥ä½¿ç”¨model.state_dict()çš„å¼•ç”¨ï¼Œç„¶ååªä¿®æ”¹LoRAå‚æ•°
        if self.is_minigpt:
            w_avg = model.state_dict()
        else:
            w_avg = copy.deepcopy(model.state_dict())
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºLoRAå‚æ•°
        def is_lora_param(key):
            return "lora" in key or any(module in key for module in self.lora_target_modules)
        
        # MiniGPTæ¨¡å‹ï¼šç®€åŒ–çš„LoRAå‚æ•°èšåˆ
        if self.is_minigpt:
            lora_param_count = 0
            for key in w_avg.keys():
                # åªå¤„ç†LoRAå‚æ•°ï¼ˆå› ä¸ºå®¢æˆ·ç«¯åªè¿”å›LoRAå‚æ•°ï¼‰
                if is_lora_param(key):
                    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å®¢æˆ·ç«¯éƒ½æœ‰è¿™ä¸ªkey
                    if all(key in w_ for w_ in w):
                        lora_param_count += 1
                        # æ”¶é›†æœ‰æ¢¯åº¦æ›´æ–°çš„å®¢æˆ·ç«¯å‚æ•°
                        updated_params = [w_[key] for i, w_ in enumerate(w) if grad_flags[i].get(key, False)]
                        
                        if updated_params:
                            # ç®€å•å¹³å‡
                            w_avg[key] = sum(updated_params) / len(updated_params)
                        # else: ä¿æŒå…¨å±€æ¨¡å‹çš„åŸå§‹å€¼
                # éLoRAå‚æ•°ï¼ˆBatchNormç­‰ï¼‰ï¼šä¿æŒå…¨å±€æ¨¡å‹çš„åŸå§‹å€¼
                # ä¸åšä»»ä½•æ“ä½œï¼Œå› ä¸ºå®¢æˆ·ç«¯æ²¡æœ‰è¿”å›è¿™äº›å‚æ•°
            
            print(f"âœ“ èšåˆå®Œæˆï¼š{lora_param_count}ä¸ªLoRAå‚æ•°å·²æ›´æ–°")
            return w_avg
        
        # ä¼ ç»Ÿæ¨¡å‹ï¼šä½¿ç”¨åŸå§‹çš„æ¢¯åº¦è¿½è¸ªèšåˆé€»è¾‘
        for key in w_avg.keys():
            if 'num_batches_tracked' in key:
                w_avg[key] = w[0][key]
                continue

            if 'running' in key:
                w_avg[key] = sum([w_[key] for w_ in w]) / len(w)
                continue

            if is_lora_param(key):
                tmp = torch.zeros_like(w_avg[key])
                count = torch.zeros_like(tmp)
                for i in range(len(w)):
                    if grad_flags[i][key]:
                        idx = self.idx_dicts[levels[i]][key]
                        idx = self.fix_idx_array(idx, w[i][key].shape)
                        tmp[idx] += w[i][key].flatten()
                        count[idx] += 1
                w_avg[key][count != 0] = tmp[count != 0]
                count[count == 0] = 1
                w_avg[key] = w_avg[key] / count
                
        return w_avg

    # æ ¹æ®è¾“å…¥çš„äºŒè¿›åˆ¶æ©ç å’Œæœ¬åœ°å½¢çŠ¶è¿”å›è¾“å‡ºå½¢çŠ¶ã€‚
    def get_idx_shape(self, inp, local_shape):
        # Return the output shape for binary mask input
        # [[1, 1, 0], [1, 1, 0], [0, 0, 0,]] -> [2, 2]
        if any([s == 0 for s in inp.shape]):
            print('Indexing error')
            raise RuntimeError

        if len(local_shape) == 4:
            dim_1 = inp.shape[2] // 2
            dim_2 = inp.shape[3] // 2
            idx_shape = (inp[:, 0, dim_1, dim_2].sum().item(),
                         inp[0, :, dim_1, dim_2].sum().item(), *local_shape[2:])
        elif len(local_shape) == 2:
            idx_shape = (inp[:, 0].sum().item(),
                         inp[0, :].sum().item())
        else:
            idx_shape = (inp.sum(),)

        return idx_shape

    # ä¿®å¤ç´¢å¼•æ•°ç»„ï¼Œç¡®ä¿å…¶å½¢çŠ¶ä¸æœ¬åœ°å½¢çŠ¶åŒ¹é…ã€‚
    def fix_idx_array(self, idx_array, local_shape):
        idx_shape = self.get_idx_shape(idx_array, local_shape)
        if all([idx_shape[i] >= local_shape[i] for i in range(len(local_shape))]):
            pass
        else:
            idx_array = idx_array[idx_array.sum(dim=1).argmax()].repeat((idx_array.shape[0], 1))
            idx_shape = self.get_idx_shape(idx_array, local_shape)

        ind_list = [slice(None)] * len(idx_array.shape)
        for i in range(len(local_shape)):

            lim = idx_array.shape[i]
            while idx_shape[i] != local_shape[i]:
                lim -= 1
                ind_list[i] = slice(0, lim)
                idx_shape = self.get_idx_shape(idx_array[tuple(ind_list)], local_shape)

        tmp = torch.zeros_like(idx_array, dtype=bool)
        tmp[tuple(ind_list)] = idx_array[tuple(ind_list)]
        idx_array = tmp

        if len(idx_array.shape) == 4:
            dim_1 = idx_array.shape[2] // 2
            dim_2 = idx_array.shape[3] // 2
            if idx_array.sum(dim=0).sum(dim=0)[0, 0] != idx_array.sum(dim=0).sum(dim=0)[dim_1, dim_2]:
                idx_array = idx_array[:, :, dim_1, dim_2].repeat(idx_array.shape[2], idx_array.shape[3], 1, 1).permute(
                    2, 3, 0, 1)
        return idx_array

    def get_local_split(self, level, scale):
        """
        ä¸ºä¸åŒèµ„æºç­‰çº§çš„å®¢æˆ·ç«¯åˆ›å»ºæœ¬åœ°æ¨¡å‹
        - å¯¹äºMiniGPTï¼šå…±äº«base modelï¼Œä»…é…ç½®ä¸åŒçš„LoRAå‚æ•°
        - å¯¹äºä¼ ç»Ÿæ¨¡å‹ï¼šä½¿ç”¨åŸå§‹çš„å‚æ•°è£å‰ªé€»è¾‘
        """
        # MiniGPTæ¨¡å‹ï¼šä½¿ç”¨å‚æ•°å…±äº« + LoRAé…ç½®
        if self.is_minigpt:
            # å¤„ç†å¼‚å¸¸levelï¼ˆ-1è¡¨ç¤ºå®¢æˆ·ç«¯ä¸åœ¨ä»»ä½•åˆ†ç»„ä¸­ï¼‰
            if level < 0 or level >= len(self.lora_configs):
                print(f"âš ï¸  è­¦å‘Šï¼šå®¢æˆ·ç«¯level {level}æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆLevel 0ï¼‰")
                level = 0
            
            # ç›´æ¥è¿”å›å…¨å±€æ¨¡å‹çš„å¼•ç”¨ï¼ˆå…±äº«base modelï¼‰
            # åœ¨è®­ç»ƒæ—¶é€šè¿‡å†»ç»“/è§£å†»ç‰¹å®šLoRAå±‚æ¥å®ç°èµ„æºè‡ªé€‚åº”
            # æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„æ˜¯å¼•ç”¨ï¼Œä¸æ˜¯å‰¯æœ¬ï¼
            print(f"âœ“ å®¢æˆ·ç«¯Level {level}: LoRAé…ç½® - å±‚æ•°={len(self.lora_configs[level]['layers'])}, rank={self.lora_configs[level]['rank']}")
            return self.global_model
        
        # ä¼ ç»Ÿæ¨¡å‹ï¼šä½¿ç”¨åŸå§‹çš„deepcopy + å‚æ•°è£å‰ªé€»è¾‘
        model = copy.deepcopy(self.global_model)

        if scale == 1:
            return model

        model_kwargs = model.stored_inp_kwargs
        if 'scale' in model_kwargs.keys():
            model_kwargs['scale'] = scale
        else:
            model_kwargs['params']['scale'] = scale

        local_model = type(self.global_model)(**model_kwargs)
        if 'bert' in str(type(local_model)):
            local_model.add_exits(model_kwargs['ee_layer_locations'])

        local_state_dict = local_model.state_dict()

        for n, p in self.global_model.state_dict().items():

            if 'num_batches_tracked' in n:
                local_state_dict[n] = p
                continue

            global_shape = p.shape
            local_shape = local_state_dict[n].shape

            if len(global_shape) != len(local_shape):
                print('Models are not alignable!')
                raise RuntimeError

            idx_array = self.fix_idx_array(self.idx_dicts[level][n], local_shape)
            local_state_dict[n] = p[idx_array].reshape(local_shape)

        local_model.load_state_dict(local_state_dict)

        return local_model


def execute_client_round(args):
    train_set, user_groups, criterion, args, batch_size, train_params, round_idx, global_model, \
    local_model, client_train_loader, level, scale, h_scale_ratio, client_idx, lora_configs = args

    if args.use_gpu:
        local_model = local_model.cuda()

    # MiniGPTæ¨¡å‹ï¼šæ ¹æ®Levelé…ç½®LoRAå±‚çš„å¯è®­ç»ƒæ€§
    if 'MiniGPT' in str(type(local_model)) and lora_configs is not None:
        # éªŒè¯levelæœ‰æ•ˆæ€§
        if level < 0 or level >= len(lora_configs):
            print(f"âš ï¸  å®¢æˆ·ç«¯{client_idx}: Level {level}æ— æ•ˆï¼Œä½¿ç”¨Level 0")
            level = 0
        
        config = lora_configs[level]
        target_layers = set(config['layers'])
        
        # ç¬¬ä¸€æ­¥ï¼šå†»ç»“æ‰€æœ‰LoRAå‚æ•°
        for name, param in local_model.named_parameters():
            if 'lora' in name.lower():
                param.requires_grad = False
        
        # ç¬¬äºŒæ­¥ï¼šåªè§£å†»ç›®æ ‡å±‚çš„LoRAå‚æ•°
        trainable_lora_count = 0
        for layer_idx in target_layers:
            # æ›´å®½æ¾çš„åŒ¹é…ï¼šåŒ¹é…åŒ…å«.layers.{layer_idx}.çš„å‚æ•°
            layer_pattern = f'.layers.{layer_idx}.'
            for name, param in local_model.named_parameters():
                if layer_pattern in name and 'lora' in name.lower():
                    param.requires_grad = True
                    trainable_lora_count += 1
        
        # æ”¶é›†å¯è®­ç»ƒå‚æ•°ï¼ˆåªåŒ…å«LoRAå‚æ•°ï¼‰
        trainable_params = [v for k, v in local_model.named_parameters() if v.requires_grad]
        
        # ä½¿ç”¨Level-Adaptiveè®­ç»ƒå‚æ•°
        level_lr = config.get('lr', 5e-6)
        level_wd = config.get('weight_decay', 0.01)
        level_grad_clip = config.get('grad_clip', 1.0)
        
        print(f"âœ“ å®¢æˆ·ç«¯{client_idx} [Level {level}]: {len(target_layers)}å±‚LoRA ({trainable_lora_count}å‚æ•°), "
              f"lr={level_lr:.1e}, grad_clip={level_grad_clip}, wd={level_wd}")
        
        # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼ˆæ›´é€‚åˆTransformerï¼‰ï¼Œä½¿ç”¨Levelå¯¹åº”çš„å­¦ä¹ ç‡
        optimizer = torch.optim.AdamW(trainable_params,
                                      lr=level_lr,
                                      weight_decay=level_wd)
        
        # å°†grad_clipä¼ é€’ç»™train_paramsï¼Œä¾›execute_epochä½¿ç”¨
        train_params['grad_clip_norm'] = level_grad_clip
    else:
        # ä¼ ç»Ÿæ¨¡å‹ï¼šä½¿ç”¨åŸå§‹çš„SGDä¼˜åŒ–å™¨
        base_params = [v for k, v in local_model.named_parameters() if 'ee_' not in k]
        exit_params = [v for k, v in local_model.named_parameters() if 'ee_' in k]

        optimizer = torch.optim.SGD([{'params': base_params},
                                     {'params': exit_params}],
                                    lr=train_params['lr'],
                                    momentum=train_params['momentum'],
                                    weight_decay=train_params['weight_decay'])

    loss = 0.0
    for epoch in range(train_params['num_epoch']):
        print(f'{client_idx}-{epoch}-{dt.datetime.now()}')
        iter_idx = round_idx
        loss = execute_epoch(local_model, client_train_loader, criterion, optimizer, iter_idx, epoch,
                             args, train_params, h_scale_ratio, level, global_model)

    print(f'Finished epochs for {client_idx}')
    
    # åªä¿å­˜LoRAå‚æ•°ï¼Œé¿å…ä¿å­˜27GBåŸºç¡€æ¨¡å‹å‚æ•°
    if args.arch == 'MiniGPTv2':
        # MiniGPT: åªä¿å­˜LoRAå‚æ•°ï¼ˆçº¦128MBï¼‰
        local_weights = {
            k: v.cpu() 
            for k, v in local_model.state_dict(keep_vars=True).items() 
            if 'lora' in k.lower()
        }
        local_grad_flags = {
            k: v.grad is not None 
            for k, v in local_model.state_dict(keep_vars=True).items() 
            if 'lora' in k.lower()
        }
        print(f"âœ“ åªä¿å­˜LoRAå‚æ•°: {len(local_weights)}ä¸ªå‚æ•° (è¿‡æ»¤æ‰åŸºç¡€æ¨¡å‹)")
    else:
        # ä¼ ç»Ÿæ¨¡å‹: ä¿å­˜æ‰€æœ‰å‚æ•°
        local_weights = {k: v.cpu() for k, v in local_model.state_dict(keep_vars=True).items()}
        local_grad_flags = {k: v.grad is not None for k, v in local_model.state_dict(keep_vars=True).items()}

    # æ¯ä¸ªå®¢æˆ·ç«¯è®­ç»ƒå®Œæ¯•åç«‹å³é‡Šæ”¾å†…å­˜
    # æ¸…ç†ä¼˜åŒ–å™¨ï¼ˆAdamWä¼šæŒæœ‰å¤§é‡momentumå’ŒvarianceçŠ¶æ€ï¼‰
    optimizer.zero_grad(set_to_none=True)
    del optimizer
    
    del local_model
    torch.cuda.empty_cache()

    return local_weights, local_grad_flags, loss